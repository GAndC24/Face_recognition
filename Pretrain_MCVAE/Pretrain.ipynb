{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-24T02:53:52.654954Z",
     "start_time": "2025-01-24T02:53:52.603170Z"
    }
   },
   "source": [
    "from HP_optimizer_MCVGAN import HP_optimizer_MCVGAN\n",
    "import torch\n",
    "from Trainer_MCVGAN import Trainer_MCVGAN\n",
    "from Model_MCVGAN import Masked_ConViT_GAN_Generator, Masked_ConViT_GAN_Discriminator\n",
    "\n",
    "# 超参数\n",
    "img_size = 128\n",
    "NP = 60\n",
    "G = 10\n",
    "select_ratio = 0.8\n",
    "L = 18\n",
    "Pc = 0.8\n",
    "Pm = 0.05\n",
    "train_mini_epochs = 15\n",
    "epochs = 1000"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T02:54:24.237742Z",
     "start_time": "2025-01-24T02:54:05.098018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 初始化 Hyperparameter optimizer\n",
    "HP_optimizer = HP_optimizer_MCVGAN(img_size=img_size, NP=NP, select_ratio=select_ratio, G=G, L=L,\n",
    "                                       Pc=Pc, Pm=Pm, train_mini_epochs=train_mini_epochs)\n",
    "\n",
    "# 获取 best Hyperparameters\n",
    "HP_best = HP_optimizer.get_best_hyperparameters()"
   ],
   "id": "e7dba4d2b6dfde16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index: 1\n",
      "img_size : 128\n",
      "lr : 6.786574058537166e-05\n",
      "weight_decay : 0.0005620466703622746\n",
      "warmup_proportion : 0.006836235181323957\n",
      "batch_size : 32\n",
      "embed_dim : 1024\n",
      "depth : 17\n",
      "num_heads : 16\n",
      "mlp_ratio : 8.0\n",
      "drop_rate : 0.5\n",
      "attn_drop_rate : 0.3\n",
      "drop_path_rate : 0.1\n",
      "local_up_to_layer : 12\n",
      "locality_strength : 1.5\n",
      "decoder_embed_dim : 768\n",
      "decoder_depth : 4\n",
      "decoder_num_heads : 8\n",
      "filter_size : 3\n",
      "num_filters : 128\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 15.99 GiB of which 0 bytes is free. Of the allocated memory 31.53 GiB is allocated by PyTorch, and 621.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 6\u001B[0m\n\u001B[0;32m      2\u001B[0m HP_optimizer \u001B[38;5;241m=\u001B[39m HP_optimizer_MCVGAN(img_size\u001B[38;5;241m=\u001B[39mimg_size, NP\u001B[38;5;241m=\u001B[39mNP, select_ratio\u001B[38;5;241m=\u001B[39mselect_ratio, G\u001B[38;5;241m=\u001B[39mG, L\u001B[38;5;241m=\u001B[39mL,\n\u001B[0;32m      3\u001B[0m                                        Pc\u001B[38;5;241m=\u001B[39mPc, Pm\u001B[38;5;241m=\u001B[39mPm, train_mini_epochs\u001B[38;5;241m=\u001B[39mtrain_mini_epochs)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# 获取 best Hyperparameters\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m HP_best \u001B[38;5;241m=\u001B[39m \u001B[43mHP_optimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_best_hyperparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python_projects\\Face_Recognition\\Pretrain_MCVAE\\HP_optimizer_MCVGAN.py:156\u001B[0m, in \u001B[0;36mHP_optimizer_MCVGAN.get_best_hyperparameters\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    140\u001B[0m     discriminator \u001B[38;5;241m=\u001B[39m Masked_ConViT_GAN_Discriminator(\n\u001B[0;32m    141\u001B[0m         img_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_size,\n\u001B[0;32m    142\u001B[0m         filter_size\u001B[38;5;241m=\u001B[39mfilter_size,\n\u001B[0;32m    143\u001B[0m         num_filters\u001B[38;5;241m=\u001B[39mnum_filters\n\u001B[0;32m    144\u001B[0m     )\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    145\u001B[0m     trainer \u001B[38;5;241m=\u001B[39m Trainer_MCVGAN(\n\u001B[0;32m    146\u001B[0m         generator\u001B[38;5;241m=\u001B[39mgenerator,\n\u001B[0;32m    147\u001B[0m         discriminator\u001B[38;5;241m=\u001B[39mdiscriminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    153\u001B[0m         epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_mini_epochs\n\u001B[0;32m    154\u001B[0m     )\n\u001B[1;32m--> 156\u001B[0m     fid \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_HP_optim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m     fid_score[i] \u001B[38;5;241m=\u001B[39m fid\n\u001B[0;32m    159\u001B[0m fitness \u001B[38;5;241m=\u001B[39m fid_score     \u001B[38;5;66;03m# 以 FID score 作为适应度值\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Python_projects\\Face_Recognition\\Pretrain_MCVAE\\Trainer_MCVGAN.py:217\u001B[0m, in \u001B[0;36mTrainer_MCVGAN.train_HP_optim\u001B[1;34m(self, index, k)\u001B[0m\n\u001B[0;32m    215\u001B[0m mask_loss, pred, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerator(X)\n\u001B[0;32m    216\u001B[0m fake_images \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerator\u001B[38;5;241m.\u001B[39munpatchify(pred)\n\u001B[1;32m--> 217\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscriminator(fake_images)\n\u001B[0;32m    218\u001B[0m extra_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_func_D(outputs, real_labels)\n\u001B[0;32m    219\u001B[0m g_loss \u001B[38;5;241m=\u001B[39m mask_loss \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m extra_loss\n",
      "File \u001B[1;32mD:\\Python_projects\\Face_Recognition\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:137\u001B[0m, in \u001B[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    135\u001B[0m opt \u001B[38;5;241m=\u001B[39m opt_ref()\n\u001B[0;32m    136\u001B[0m opt\u001B[38;5;241m.\u001B[39m_opt_called \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__get__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mopt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__class__\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python_projects\\Face_Recognition\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    482\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    483\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    484\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    485\u001B[0m             )\n\u001B[1;32m--> 487\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    490\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Python_projects\\Face_Recognition\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     89\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[1;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     93\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[1;32mD:\\Python_projects\\Face_Recognition\\.venv\\Lib\\site-packages\\torch\\optim\\adamw.py:220\u001B[0m, in \u001B[0;36mAdamW.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    207\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m cast(Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m], group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    209\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[0;32m    210\u001B[0m         group,\n\u001B[0;32m    211\u001B[0m         params_with_grad,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    217\u001B[0m         state_steps,\n\u001B[0;32m    218\u001B[0m     )\n\u001B[1;32m--> 220\u001B[0m     \u001B[43madamw\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    221\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    222\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    223\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    224\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    225\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    226\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    227\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    228\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    229\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    230\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    232\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    233\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    234\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    235\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    236\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    238\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    239\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    241\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32mD:\\Python_projects\\Face_Recognition\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python_projects\\Face_Recognition\\.venv\\Lib\\site-packages\\torch\\optim\\adamw.py:782\u001B[0m, in \u001B[0;36madamw\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    779\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    780\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adamw\n\u001B[1;32m--> 782\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    783\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    784\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    785\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    786\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    787\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    788\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    789\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    801\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python_projects\\Face_Recognition\\.venv\\Lib\\site-packages\\torch\\optim\\adamw.py:606\u001B[0m, in \u001B[0;36m_multi_tensor_adamw\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001B[0m\n\u001B[0;32m    604\u001B[0m     exp_avg_sq_sqrt \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001B[0;32m    605\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 606\u001B[0m     exp_avg_sq_sqrt \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_foreach_sqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_exp_avg_sqs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    608\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001B[0;32m    609\u001B[0m torch\u001B[38;5;241m.\u001B[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 15.99 GiB of which 0 bytes is free. Of the allocated memory 31.53 GiB is allocated by PyTorch, and 621.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lr = HP_best[0]\n",
    "warmup_proportion = HP_best[1]\n",
    "weight_decay = HP_best[2]\n",
    "batch_size = HP_best[3]\n",
    "embed_dim = HP_best[4]\n",
    "depth = HP_best[5]\n",
    "num_heads = HP_best[6]\n",
    "mlp_ratio = HP_best[7]\n",
    "drop_rate = HP_best[8]\n",
    "attn_drop_rate = HP_best[9]\n",
    "drop_path_rate = HP_best[10]\n",
    "local_up_to_layer = HP_best[11]\n",
    "locality_strength = HP_best[12]\n",
    "decoder_embed_dim = HP_best[13]\n",
    "decoder_depth = HP_best[14]\n",
    "decoder_num_heads = HP_best[15]\n",
    "filter_size = HP_best[16]\n",
    "num_filters = HP_best[17]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 使用 cuda\n",
    "\n",
    "# 初始化 generator\n",
    "generator = Masked_ConViT_GAN_Generator(\n",
    "    img_size=img_size,\n",
    "    embed_dim=embed_dim,\n",
    "    depth=depth,\n",
    "    num_heads=num_heads,\n",
    "    mlp_ratio=mlp_ratio,\n",
    "    drop_rate=drop_rate,\n",
    "    attn_drop_rate=attn_drop_rate,\n",
    "    drop_path_rate=drop_path_rate,\n",
    "    local_up_to_layer=local_up_to_layer,\n",
    "    locality_strength=locality_strength,\n",
    "    decoder_embed_dim=decoder_embed_dim,\n",
    "    decoder_depth=decoder_depth,\n",
    "    decoder_num_heads=decoder_num_heads\n",
    ").to(device)\n",
    "\n",
    "# 初始化 discriminator\n",
    "discriminator = Masked_ConViT_GAN_Discriminator(\n",
    "    img_size=img_size,\n",
    "    filter_size=filter_size,\n",
    "    num_filters=num_filters\n",
    ").to(device)\n",
    "\n",
    "# 初始化 trainer\n",
    "trainer = Trainer_MCVGAN(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    lr=lr,\n",
    "    warmup_proportion=warmup_proportion,\n",
    "    weight_decay=weight_decay,\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_generator()"
   ],
   "id": "a124bcf3b19fea46"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
